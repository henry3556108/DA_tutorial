{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f52186-2831-4912-8e68-4f706668ce53",
   "metadata": {},
   "source": [
    "## 距離計算\n",
    "\n",
    "### 1. 歐式距離（Euclidean Distance）\n",
    "歐式距離是最常見的距離計量方法之一，通常被用來計算兩點之間的直線距離。在二維空間中，它就是根據畢氏定理計算的距離。在 n 維空間中，歐式距離的計算公式為：\n",
    "\n",
    "$ \\text{Euclidean distance} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2} $\n",
    "\n",
    "其中 $x_i$ 和 $y_i$ 是兩個點在第 i 維的座標。\n",
    "\n",
    "### 2. 曼哈頓距離（Manhattan Distance）\n",
    "曼哈頓距離也稱為城市街區距離（Taxicab geometry），它是指在正交坐標系中，兩點在標準座標軸上的絕對軸距離總和。這種距離的計算方式如同在一個棋盤或城市街道網格中行走一樣。在 n 維空間中，曼哈頓距離的計算公式為：\n",
    "\n",
    "$ \\text{Manhattan distance} = \\sum_{i=1}^{n} |x_i - y_i| $\n",
    "\n",
    "### 3. 余弦相似度（Cosine Similarity）\n",
    "余弦相似度並不直接計算兩個點之間的距離，而是計算它們的夾角的余弦值，這常用於衡量兩個向量方向的相似度。余弦值越接近 1 表示角度越小，方向越相似。余弦相似度的計算公式為：\n",
    "\n",
    "$ \\text{Cosine similarity} = \\frac{\\sum_{i=1}^{n} (x_i \\cdot y_i)}{\\sqrt{\\sum_{i=1}^{n} x_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} y_i^2}} $\n",
    "\n",
    "通常，在處理文本數據或任何高維度且稀疏的數據時，余弦相似度特別有用，因為它更多地關注向量在方向上的相似性而不是向量的大小。\n",
    "\n",
    "這三種方法在不同的數據集和應用中各有優劣，選擇合適的距離計量方法可以提高 KNN 等基於距離的機器學習算法的性能和準確性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea4c3b-965a-47df-a1ec-544c7d01d3ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2de0e1-38be-4617-a4fe-41a1879f9d5f",
   "metadata": {},
   "source": [
    "## K近鄰法（K-Nearest Neighbors, KNN）\n",
    "K近鄰法（K-Nearest Neighbors, KNN）是一種基於實例的學習方法，主要用於分類和回歸任務。它不需要明確的模型訓練階段，而是直接根據訓練數據集中最接近的K個鄰居的標籤來預測新樣本的標籤。KNN是機器學習中一個直觀且簡單的算法。\n",
    "\n",
    "### KNN的工作原理\n",
    "\n",
    "1. **選擇K值**：確定一個數字K，表示選取的鄰居數量。K的選擇通常基於實際應用，可能需要通過交叉驗證來優化。\n",
    "\n",
    "2. **距離計算**：對於每個待預測的新樣本，計算它與訓練集中所有樣本的距離。常用的距離計算方法包括歐氏距離、曼哈頓距離等。\n",
    "\n",
    "3. **確定最近的K個鄰居**：根據計算出的距離，選擇最近的K個訓練樣本作為鄰居。\n",
    "\n",
    "4. **進行預測**：對於分類任務，K個最近鄰居中最常見的類別將被選為新樣本的類別。對於回歸任務，取K個鄰居的目標值的平均數作為預測結果。\n",
    "\n",
    "### 優點\n",
    "\n",
    "- **簡單直觀**：KNN算法非常容易理解和實現，對於一些基本的分類和回歸問題可以很快得到不錯的結果。\n",
    "- **無需訓練階段**：KNN在使用前不需要進行模型的訓練，是一種\"懶惰學習\"（lazy learning）算法，因此適合於那些樣本數據動態更新的場景。\n",
    "- **適用於多類別問題**：KNN可以處理多類別的分類任務，而無需進行特殊的處理。\n",
    "\n",
    "### 缺點\n",
    "\n",
    "- **計算量大**：因為每次對新樣本進行預測時都需要計算與所有訓練樣本的距離，當訓練集很大時，這會導致很大的計算成本。\n",
    "- **對異常值敏感**：KNN對異常值很敏感，因為異常值會對鄰居的選擇產生很大的影響。\n",
    "- **特徵縮放依賴**：KNN的性能受到特徵縮放的影響很大。如果特徵間的尺度差異大，則在距離計算時可能會導致某些特徵被高估，從而影響最終的預測效果。因此在使用KNN前，通常需要對數據進行標準化或歸一化處理。\n",
    "\n",
    "這種模型適用於那些樣本量不是特別大，且特徵預處理得當的數據集。對於大規模數據集，可能需要考慮更適合的算法或使用一些優化技術，如樹結構或近似最近鄰搜索等。\n",
    "\n",
    "- [延伸閱讀](https://ithelp.ithome.com.tw/m/articles/10269826)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d5ad80-913f-4edd-ab16-d6d674d2f924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "預測準確率為： 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 載入 iris 資料集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 將資料集分割為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建 KNN 模型，設置鄰居數為 3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# 訓練模型\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 使用測試集進行預測\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "# 計算並打印準確率\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"預測準確率為：\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc3f619-45e9-43cf-8939-a3d3260c3909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳的 k 值： {'n_neighbors': 3}\n",
      "交叉驗證的最佳準確率： 0.9583333333333334\n",
      "測試集的準確率： 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 載入 iris 資料集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 將資料集分割為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 創建 KNN 模型\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# 設定想要測試的 k 值\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 11]}\n",
    "\n",
    "# 設定 GridSearchCV，使用 5 折交叉驗證\n",
    "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# 執行 grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 輸出最佳的 k 值和對應的準確率\n",
    "print(\"最佳的 k 值：\", grid_search.best_params_)\n",
    "print(\"交叉驗證的最佳準確率：\", grid_search.best_score_)\n",
    "\n",
    "# 使用最佳的 k 值對測試集進行預測\n",
    "best_knn = grid_search.best_estimator_\n",
    "predictions = best_knn.predict(X_test)\n",
    "\n",
    "# 計算測試集的準確率\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"測試集的準確率：\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef66b12-4b3f-4d98-9f5b-32e2496d5bd5",
   "metadata": {},
   "source": [
    "## KNN 迴歸\n",
    "- [延伸閱讀](https://tomohiroliu22.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98%E7%B3%BB%E5%88%97-47-k-%E9%84%B0%E8%BF%91%E6%BC%94%E7%AE%97%E6%B3%95%E5%9B%9E%E6%AD%B8-k-nearest-neighbor-regression-e3cb8fec4d60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd75e2d-b95a-4708-882f-2c7117572c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 載入 diabetes 資料集\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# 將資料集分割為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 創建 KNN 迴歸模型，設置鄰居數為 5\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# 訓練模型\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# 使用測試集進行預測\n",
    "predictions = knn_regressor.predict(X_test)\n",
    "\n",
    "# 計算並打印均方誤差（MSE）\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"均方誤差 (MSE) 為：\", mse)\n",
    "\n",
    "# 計算並打印均方根誤差（RMSE）\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"均方根誤差 (RMSE) 為：\", rmse)\n",
    "\n",
    "# 計算並打印 R² 分數\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R² 分數為：\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde6a29-ddff-4d55-a022-f43d65d64b9a",
   "metadata": {},
   "source": [
    "## 練習\n",
    "Tunning 上面的這個回歸模型，並且最終告訴我最佳的超參數組合為多少，該組合最終的 R2 score 為多少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f25911-983f-4c44-bb05-c8cc69ac6655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea509dc8-e27b-4803-ac31-d77e130b837b",
   "metadata": {},
   "source": [
    "## 練習\n",
    "嘗試使用 Titanic 資料集來做 KNN 練習，訓練出你覺得最好的模型，並且告訴我你的評估方式與最終評估的指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89349fd6-00f5-4cf8-a9ac-43a43e464857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
