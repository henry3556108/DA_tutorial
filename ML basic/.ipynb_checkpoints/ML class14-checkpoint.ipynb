{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02230fd8-db58-4515-bc8f-13e0e1b940c7",
   "metadata": {},
   "source": [
    "# Tunning超參數調整步驟\n",
    "\n",
    "## 步驟一：切分資料集\n",
    "\n",
    "1. 將資料集切分為訓練集 (training set) 和測試集 (test set)。\n",
    "2. 測試集保留不使用，用於最後評估模型的泛化性。\n",
    "\n",
    "## 步驟二：使用交叉驗證切分訓練集\n",
    "\n",
    "1. 將訓練集再使用交叉驗證 (cross-validation) 切分為 K 份訓練/驗證集。\n",
    "2. 每份訓練/驗證集包含 K-1 份訓練集和 1 份驗證集。\n",
    "\n",
    "## 步驟三：超參數搜索\n",
    "\n",
    "1. 使用網格搜索 (grid search) 或隨機搜索 (random search) 等超參數搜索方法。\n",
    "2. 在訓練/驗證集上訓練模型並評估性能。\n",
    "\n",
    "## 步驟四：選出最佳超參數\n",
    "\n",
    "1. 根據驗證集的性能，選出最佳的超參數組合。\n",
    "\n",
    "## 步驟五：訓練最終模型\n",
    "\n",
    "1. 使用最佳的超參數組合，對全部訓練集進行訓練。\n",
    "\n",
    "## 步驟六：評估模型性能\n",
    "\n",
    "1. 使用測試集評估模型的性能。\n",
    "\n",
    "## 注意事項\n",
    "\n",
    "* 在切分資料集時，需要確保訓練集和測試集具有相同的分布。\n",
    "* 在使用交叉驗證時，需要確保每個子集都包含資料集的代表性樣本。\n",
    "* 在超參數搜索時，需要考慮模型的複雜度和泛化性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d81958-d1ae-444f-aa45-5b9a911533c8",
   "metadata": {},
   "source": [
    "## 交叉驗證\n",
    "\n",
    "交叉驗證是一種統計方法，用於評估機器學習模型的泛化能力（generalization ability），即其對未見過數據的預測能力。這種方法涉及將數據集分割成多個小子集，並將這些子集輪流用作訓練和測試數據。\n",
    "\n",
    "### 交叉驗證的目的\n",
    "模型評估（Model Evaluation）：交叉驗證的主要目的是提供一種估計機器學習模型在新數據上表現的方法。這尤其對於避免模型過擬合（overfitting）至關重要，過擬合發生時，模型對訓練數據的學習過於徹底，從而對新數據的泛化能力差。\n",
    "\n",
    "模型選擇（Model Selection）：交叉驗證允許研究人員比較不同模型的性能，並選擇最佳的模型設置。例如，研究人員可以用它來決定模型參數或選擇學習算法。\n",
    "\n",
    "![](https://ithelp.ithome.com.tw/upload/images/20220323/20107247Ez4WXEkZTq.png)\n",
    "- [延伸閱讀 不同主的資料切割方法](https://ithelp.ithome.com.tw/articles/10279240)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7d7d3-32ef-4218-819a-156e59de8db8",
   "metadata": {},
   "source": [
    "## K-fold\n",
    "\n",
    "K-fold 交叉驗證是一種常用的模型評估方法，尤其在機器學習和統計模型中。這種方法能夠充分利用有限的數據來評估模型的性能，同時避免過擬合。\n",
    "\n",
    "### 基本原理：\n",
    "在 K-fold 交叉驗證中，原始數據集被隨機分割成 K 個子集（通常稱為“折”或“folds”）。這些子集盡量保持大小相等。交叉驗證的過程中，每次會將其中的一個子集作為測試集，其餘的 K-1 個子集合併後作為訓練集。這個過程會重複 K 次，每次選擇不同的子集作為測試集。\n",
    "\n",
    "### 步驟詳解：\n",
    "1. **數據分割**：將整個數據集隨機分割成 K 個子集。\n",
    "2. **迭代訓練與評估**：每次迭代中，選擇一個子集作為測試集，其餘作為訓練集。訓練模型後，使用該次迭代的測試集來評估模型。\n",
    "3. **性能匯總**：在 K 次迭代過程中收集模型的性能指標，如準確率、F1 分數等。最後，計算這些性能指標的平均值來得到模型的整體表現。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54c3bab-60f9-4fc2-8e5e-b5c2cf7fa5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均準確率：0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 載入 iris 資料集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 設定 KFold 參數\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 初始化模型\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# 用來存儲每次的準確率\n",
    "accuracies = []\n",
    "\n",
    "# 執行 K-fold 交叉驗證\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # 訓練模型\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 預測測試集\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # 計算並存儲準確率\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# 計算平均準確率\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"平均準確率：{average_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df60af62-b65a-4ea0-9e63-63c427100a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'n_estimators': 50}\n",
      "0.9421296296296297\n",
      "[2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 載入 iris 資料集\n",
    "iris = load_iris()\n",
    "\n",
    "# 將資料集分割為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=0)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring=\"f1_macro\") # 這邊的 cv 就是在指資料及要切成幾塊\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(clf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec7d379-ab6f-444a-a98b-9aa60f50b7c1",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "- 使用 RandomizedSearchCV 隨機選取 `n_iter` 次指定的超參數範圍內的值來訓練模型，從而可以快速找到比較好的參數組合。這種方法特別適合當參數空間大或者計算資源有限時使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3be63cc4-a8f9-43e2-a06e-9cd59bb271c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'n_estimators': 77}\n",
      "0.9717034521788341\n",
      "[2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import randint\n",
    "\n",
    "# 載入 iris 資料集\n",
    "iris = load_iris()\n",
    "\n",
    "# 將資料集分割為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=0)\n",
    "\n",
    "# 定義參數的分布\n",
    "param_dist = {\n",
    "    \"n_estimators\": randint(10, 200),  # 給定一個範圍，隨機搜索將在這範圍內選取值\n",
    "    \"max_depth\": randint(3, 10)         # max_depth 同樣給定範圍\n",
    "}\n",
    "\n",
    "# 使用 RandomizedSearchCV 進行隨機搜索\n",
    "clf = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_dist, n_iter=5, cv=5, scoring=\"f1_macro\", random_state=0)\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(clf.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4b6c7-cb4f-4486-8d42-600fe72fe300",
   "metadata": {},
   "source": [
    "- [tunning 參考](https://www.kaggle.com/code/rashikrahmanpritom/heart-attack-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3607f31-6e21-4ff4-8e4d-22379def268c",
   "metadata": {},
   "source": [
    "## 練習\n",
    "- 使用指定的資料集，並且使用上述的兩種方法來去搜尋出各自最佳的超參數設定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df55d93-bc5c-4fdf-8658-09df8341c29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
