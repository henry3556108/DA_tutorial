{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35553b47-7ba9-4cde-bf38-1472647ddca4",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "集成學習是一種將多個模型組合成一個系統的方法，其目的是透過結合多個弱模型來構建一個更強大的模型，提升預測準確性和穩定性。在科學競賽和實務應用中，集成學習通常是有效的方式之一。\n",
    "\n",
    "### 集成學習的優點\n",
    "\n",
    "* **提高準確率**：集成學習可以通過組合多個模型的優勢，從而提高整體準確率。\n",
    "* **降低過擬合風險**：集成學習可以通過多個模型的相互驗證，從而降低過擬合風險。\n",
    "* **提高魯棒性**：集成學習可以通過多個模型的相互補充，從而提高魯棒性。\n",
    "\n",
    "\n",
    "[延伸閱讀](https://ithelp.ithome.com.tw/m/articles/10271882)\n",
    "\n",
    "[延伸閱讀](https://hackmd.io/@shaoeChen/B1CoXxvmm/https%3A%2F%2Fhackmd.io%2Fs%2FHJ5IdDoSE)\n",
    "\n",
    "[延伸閱讀](https://medium.com/@imirene/python%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E6%B7%B7%E5%90%88%E6%B3%9B%E5%8C%96-blending-%E8%88%87%E5%A0%86%E7%96%8A%E6%B3%9B%E5%8C%96-stacking-e3f4c73fe11d)\n",
    "\n",
    "[延伸閱讀 比較難 看完前面在看這個](https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ensemble-learning%E4%B9%8Bbagging-boosting%E5%92%8Cadaboost-af031229ebc3)\n",
    "\n",
    "[bagging boosting](https://www.youtube.com/watch?v=tjy0yL1rRRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6072e-9e1c-4e1e-88f1-709c1acc130c",
   "metadata": {},
   "source": [
    "## 資料集成 (Data Ensemble)\n",
    "- 定義：資料集成是指利用相同模型，對不同的資料進行多次評估，最終合併多次評估結果來提高預測準確度。透過這種方式，模型可以更加穩定，避免對單一資料集的過度擬合。\n",
    "\n",
    "1. 裝袋法 (Bagging)：將資料隨機分成多個子集，每個子集用相同的模型進行訓練，最終將所有模型的預測結果進行平均或投票。例如，隨機森林（Random Forest）是一個典型的裝袋法應用。\n",
    "2. 提升法 (Boosting)：連續訓練多個模型，每個模型都基於前一個模型的錯誤進行調整，並逐步改進預測性能。梯度提升機（Gradient Boosting Machine, GBM）是提升法的經典例子。\n",
    "\n",
    "## 模型集成 (Model Ensemble)\n",
    "- 定義：模型集成是使用同一份資料，通過多個不同模型進行預測，並將這些模型的結果合成為最終的預測。這樣做可以利用不同模型的優勢，達到更穩健的結果。\n",
    "\n",
    "1. 混合泛化 (Blending)：使用不同的模型進行預測，然後將這些模型的預測結果當作特徵，輸入到另一個模型中，從而生成最終預測結果。\n",
    "2. 堆疊泛化 (Stacking)：將不同模型的預測結果作為新的特徵，訓練一個新的模型來學習如何組合這些結果。這種方法可以更好地捕捉不同模型之間的互補性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd4284-5a19-45d5-8754-c72a75022309",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregating)\n",
    "\n",
    "是集成學習中的一種特殊方法，它通過對多個模型的預測結果進行- 透過隨機抽樣方式對訓練資料進行重複抽樣，然後訓練多個模型並將它們的預測結果平均（或投票）來減少模型的變異性。\n",
    "- 代表算法：隨機森林（Random Forest）。\n",
    "- 主要目標是降低模型的方差，進而提升穩定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121a6325-876f-4b64-bc8d-645e4f03fc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 讀取 iris 資料集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 切分訓練集與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 使用隨機森林進行訓練\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6535c5-4f12-474a-bec4-a98d98565b84",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "- 通過逐步建立多個模型，每個模型試圖糾正前一個模型的錯誤。Boosting 將每個模型的預測結果加權，最終形成一個強大的預測器。\n",
    "- 代表算法：AdaBoost、Gradient Boosting、XGBoost。\n",
    "- 主要目標是降低模型的偏差，強化模型的準確度。\n",
    "\n",
    "- [adaboost 補充](https://www.youtube.com/watch?v=LsK-xG1cLYA&t=30s&pp=ygUSc3RhdHF1ZXN0IGFkYWJvb3N0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "479d7465-94e5-4c15-9fc4-64b33a2a88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost 模型的準確度: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 引入必要的套件\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 讀取 Iris 資料集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 將資料集分為訓練集與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 建立基模型 - 使用決策樹作為弱分類器\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# 初始化 AdaBoost 模型\n",
    "adaboost = AdaBoostClassifier(base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# 訓練模型\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "# 預測\n",
    "y_pred = adaboost.predict(X_test)\n",
    "\n",
    "# 計算準確度\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"AdaBoost 模型的準確度: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e68d149-774a-4aa6-9c02-afa2e952826c",
   "metadata": {},
   "source": [
    "## Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ec91d0-1ec3-4ec7-af08-64db9079d276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blending Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 讀取 iris 資料集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 切分訓練集與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 第一層模型: 使用支持向量機和邏輯回歸\n",
    "model1 = SVC(probability=True)\n",
    "model2 = LogisticRegression()\n",
    "\n",
    "# 訓練第一層模型\n",
    "model1.fit(X_train, y_train)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# 使用第一層模型預測測試集的機率\n",
    "pred1 = model1.predict_proba(X_test)\n",
    "pred2 = model2.predict_proba(X_test)\n",
    "\n",
    "# 將第一層模型的預測結果進行融合 (簡單平均)\n",
    "blended_pred = (pred1 + pred2) / 2\n",
    "blended_pred_final = np.argmax(blended_pred, axis=1)\n",
    "\n",
    "# 評估結果\n",
    "accuracy = accuracy_score(y_test, blended_pred_final)\n",
    "print(f\"Blending Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90db73-887a-456f-bb92-e500c0a13bae",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "- 不同於 Bagging 和 Boosting，Stacking 是將多個模型的預測結果作為新的輸入，並使用另一個模型進行最終的預測。它允許結合不同類型的模型（例如決策樹、SVM、神經網絡）來提升預測效能。\n",
    "- 通常分為兩層：第一層是基模型（base models），第二層是元學習模型（meta-learner）。\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:1330/format:webp/1*D6Vxo2BJqQegET9VsnN50A.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf45d53c-8f50-41d1-85a0-9f3cd412d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 讀取 iris 資料集\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# 切分訓練集與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 定義基模型\n",
    "base_estimators = [\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('svm', SVC(probability=True))\n",
    "]\n",
    "\n",
    "# 定義堆疊分類器，最終模型為邏輯回歸\n",
    "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=LogisticRegression())\n",
    "\n",
    "# 訓練模型\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# 預測與評估\n",
    "y_pred = stacking_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20b8c0-325a-4d59-84bd-e9c3add5da8a",
   "metadata": {},
   "source": [
    "### 1. **裝袋法 (Bagging)**\n",
    "   - **代表模型**：隨機森林 (Random Forest)\n",
    "   - **優點**：\n",
    "     - **減少過擬合**：透過隨機抽樣子集來訓練模型，可以減少單一模型的過擬合情況，並提高預測的穩定性。\n",
    "     - **並行訓練**：每個模型獨立訓練，可以並行化處理，減少訓練時間。\n",
    "     - **適合高方差模型**：如決策樹等模型，裝袋法可以有效減少模型的不穩定性。\n",
    "   - **缺點**：\n",
    "     - **不擅長處理偏差**：如果基模型本身的偏差較高（如弱分類器），則可能無法充分提高準確性。\n",
    "     - **結果可解釋性較差**：隨機森林集成了多棵決策樹，單一結果的可解釋性變差。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **提升法 (Boosting)**\n",
    "   - **代表模型**：梯度提升機 (Gradient Boosting Machine, GBM)、XGBoost、LightGBM\n",
    "   - **優點**：\n",
    "     - **減少偏差**：提升法逐步修正前一個模型的錯誤，因此非常擅長減少模型的偏差，從而提高整體準確性。\n",
    "     - **高準確度**：提升法通常比裝袋法能夠獲得更高的預測準確度，特別是在處理具有複雜模式的資料集時。\n",
    "     - **自適應性強**：每個新模型的訓練會集中在先前模型錯誤較大的樣本上，自動修正錯誤。\n",
    "   - **缺點**：\n",
    "     - **計算量大**：由於是順序訓練，每個模型依賴於前一個模型，因此不能並行處理，訓練速度較慢。\n",
    "     - **容易過擬合**：如果參數調整不當，提升法可能會過度擬合訓練資料。\n",
    "     - **複雜度較高**：模型調參較複雜，對超參數（如學習率、弱分類器數量等）較為敏感。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **混合泛化 (Blending)**\n",
    "   - **優點**：\n",
    "     - **簡單易實現**：Blending 通常只需要多個模型的預測結果做簡單的合成，不需要進行多層的訓練，實現上較為簡單。\n",
    "     - **降低過擬合風險**：通過多個模型的結果加權或平均，可以降低單一模型的過擬合風險。\n",
    "     - **結果更穩定**：利用不同模型的優勢，能在一定程度上提高模型的泛化能力。\n",
    "   - **缺點**：\n",
    "     - **性能有限**：Blending 的效果依賴於所選模型的性能，如果基礎模型性能較差，Blending 無法顯著提升結果。\n",
    "     - **無法深度捕捉模型之間的互補性**：與 Stacking 相比，Blending 無法通過更高層次的模型來深入學習不同模型之間的關係，效果可能不如 Stacking。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **堆疊泛化 (Stacking)**\n",
    "   - **優點**：\n",
    "     - **高度靈活**：可以同時利用多個不同類型的模型，通過第二層或多層模型學習更複雜的模式，最大化不同模型的互補性。\n",
    "     - **提升預測性能**：由於能夠在高層次上學習如何組合基礎模型的輸出，Stacking 通常能夠比單一模型和 Blending 更精確。\n",
    "   - **缺點**：\n",
    "     - **較難調參**：需要選擇多個不同的基礎模型，並且需要找到最佳的高層模型來組合這些結果，調參過程較為複雜。\n",
    "     - **過擬合風險**：如果高層模型的設置不當，Stacking 也可能過擬合於訓練數據。\n",
    "     - **計算成本高**：需要訓練多個不同模型並進行二次預測，計算量和訓練時間較大。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "108b948f-c6e0-4335-984a-4708b1a994c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 載入 iris 資料集\u001b[39;00m\n\u001b[1;32m     11\u001b[0m iris \u001b[38;5;241m=\u001b[39m load_iris()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "# 匯入必要的庫\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import shap\n",
    "\n",
    "# 載入 iris 資料集\n",
    "iris = load_iris()\n",
    "\n",
    "# 將資料集分割為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=0)\n",
    "\n",
    "# 建立 RFT 和 Adaboost 模型\n",
    "rft = RandomForestClassifier(n_estimators=100)\n",
    "adaboost = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "# 建立 stacking 模型\n",
    "stacking = StackingClassifier(estimators=[('rft', rft), ('adaboost', adaboost)], final_estimator=XGBClassifier())\n",
    "\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "explainer = shap.explainers.Exact(stacking.predict_proba, X_train)\n",
    "\n",
    "shap_values = explainer(X_train)\n",
    "# shap.plots.bar(shap_values)\n",
    "# shap_values\n",
    "shap_values = shap_values[..., 1] # 我覺得寫法太特別了 所以斟酌使用\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6f8dcc-54be-43ee-b153-77293289513c",
   "metadata": {},
   "source": [
    "- [shap官方文件](https://shap.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e06cd-f29e-4ab1-a9ee-25e5a5c0616c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
